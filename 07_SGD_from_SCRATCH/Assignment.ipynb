{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7eiDWcM_MC3H"
   },
   "source": [
    "# <font color='red'>Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfe2NTQtLq11"
   },
   "source": [
    "**There will be some functions that start with the word \"grader\" ex: grader_weights(), grader_sigmoid(), grader_logloss() etc, you should not change those function definition.<br><br>Every Grader function has to return True.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fk5DSPCLxqT-"
   },
   "source": [
    "<font color='red'> Importing packages</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42Et8BKIxnsp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpSk3WQBx7TQ"
   },
   "source": [
    "<font color='red'>Creating custom dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsMp0oWzx6dv"
   },
   "outputs": [],
   "source": [
    "# please don't change random_state\n",
    "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
    "# make_classification is used to create custom dataset \n",
    "# Please check this link (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L8W2fg1cyGdX",
    "outputId": "029d4c84-03b2-4143-a04c-34ff49c88890"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 15), (50000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x99RWCgpqNHw"
   },
   "source": [
    "<font color='red'>Splitting data into train and test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Kh4dBfVyJMP"
   },
   "outputs": [],
   "source": [
    "#please don't change random state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gONY1YiDq7jD"
   },
   "outputs": [],
   "source": [
    "# Standardizing the data.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0DR_YMBsyOci",
    "outputId": "732014d9-1731-4d3f-918f-a9f5255ee149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 15), (37500,), (12500, 15), (12500,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BW4OHswfqjHR"
   },
   "source": [
    "# <font color='red' size=5>SGD classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "3HpvTwDHyQQy",
    "outputId": "5729f08c-079a-4b17-bf51-f9aeb5abb13b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha : float\n",
    "# Constant that multiplies the regularization term. \n",
    "\n",
    "# eta0 : double\n",
    "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
    "\n",
    "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf\n",
    "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "YYaVyQ2lyXcr",
    "outputId": "dc0bf840-b37e-4552-e513-84b64f6c64c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.70, NNZs: 15, Bias: -0.501317, T: 37500, Avg. loss: 0.552526\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.04, NNZs: 15, Bias: -0.752393, T: 75000, Avg. loss: 0.448021\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.26, NNZs: 15, Bias: -0.902742, T: 112500, Avg. loss: 0.415724\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.43, NNZs: 15, Bias: -1.003816, T: 150000, Avg. loss: 0.400895\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.55, NNZs: 15, Bias: -1.076296, T: 187500, Avg. loss: 0.392879\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.65, NNZs: 15, Bias: -1.131077, T: 225000, Avg. loss: 0.388094\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.73, NNZs: 15, Bias: -1.171791, T: 262500, Avg. loss: 0.385077\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.80, NNZs: 15, Bias: -1.203840, T: 300000, Avg. loss: 0.383074\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.86, NNZs: 15, Bias: -1.229563, T: 337500, Avg. loss: 0.381703\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.90, NNZs: 15, Bias: -1.251245, T: 375000, Avg. loss: 0.380763\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.94, NNZs: 15, Bias: -1.269044, T: 412500, Avg. loss: 0.380084\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.98, NNZs: 15, Bias: -1.282485, T: 450000, Avg. loss: 0.379607\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.01, NNZs: 15, Bias: -1.294386, T: 487500, Avg. loss: 0.379251\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.03, NNZs: 15, Bias: -1.305805, T: 525000, Avg. loss: 0.378992\n",
      "Total training time: 0.10 seconds.\n",
      "Convergence after 14 epochs took 0.10 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=15, verbose=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train) # fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "EAfkVI6GyaRO",
    "outputId": "bc88f920-6531-4106-9b4c-4dabb6d72b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.89007184,  0.63162363, -0.07594145,  0.63107107, -0.38434375,\n",
       "          0.93235243, -0.89573521, -0.07340522,  0.40591417,  0.4199991 ,\n",
       "          0.24722143,  0.05046199, -0.08877987,  0.54081652,  0.06643888]]),\n",
       " (1, 15),\n",
       " array([-1.30580538]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.coef_.shape, clf.intercept_\n",
    "#clf.coef_ will return the weights\n",
    "#clf.coef_.shape will return the shape of weights\n",
    "#clf.intercept_ will return the intercept term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-CcGTKgsMrY"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1_8bdzitDlM"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "1.  We will be giving you some functions, please write code in that functions only.\n",
    "\n",
    "2.  After every function, we will be giving you expected output, please make sure that you get that output. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zU2Y3-FQuJ3z"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "* Initialize the weight_vector and intercept term to zeros (Write your code in <font color='blue'>def initialize_weights()</font>)\n",
    "\n",
    "* Create a loss function (Write your code in <font color='blue'>def logloss()</font>) \n",
    "\n",
    " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
    "- for each epoch:\n",
    "\n",
    "    - for each batch of data points in train: (keep batch size=1)\n",
    "\n",
    "        - calculate the gradient of loss function w.r.t each weight in weight vector (write your code in <font color='blue'>def gradient_dw()</font>)\n",
    "\n",
    "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>\n",
    "\n",
    "        - Calculate the gradient of the intercept (write your code in <font color='blue'> def gradient_db()</font>) <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
    "\n",
    "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$\n",
    "\n",
    "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
    "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
    "\n",
    "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $\n",
    "    - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
    "    - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
    "        you can stop the training\n",
    "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZR_HgjgS_wKu"
   },
   "source": [
    "<font color='blue'>Initialize weights </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GecwYV9fsKZ9"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(dim):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    #initialize the weights to zeros array of (1,dim) dimensions\n",
    "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
    "    #initialize bias to zero\n",
    "    \n",
    "    w = np.zeros_like(dim)  # Randomly initializing weights\n",
    "    b = np.zeros_like((1))  # Random intercept value\n",
    "\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7I6uWBRsKc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.39348337 -0.19771903 -0.15037836 -0.21528098 -1.28594363 -0.66049132\n",
      "  0.04140556 -0.22680269 -0.511055   -0.42871073  0.4210912   0.22560347\n",
      " -0.6624427  -0.68888516  0.56015427]\n",
      "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "print(dim)\n",
    "w,b = initialize_weights(dim)\n",
    "print('w =',(w))\n",
    "print('b =',str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4MI5SAjP9ofN"
   },
   "source": [
    "<font color='cyan'>Grader function - 1 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pv1llH429wG5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "def grader_weights(w,b):\n",
    "  assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
    "  return True\n",
    "grader_weights(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QN83oMWy_5rv"
   },
   "source": [
    "<font color='blue'>Compute sigmoid </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPv4NJuxABgs"
   },
   "source": [
    "$sigmoid(z)= 1/(1+exp(-z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAfmQF47_Sd6"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    # compute sigmoid(z) and return\n",
    "    sigmoid = 1/(1+math.exp(-z))\n",
    "\n",
    "    return sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YrGDwg3Ae4m"
   },
   "source": [
    "<font color='cyan'>Grader function - 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_JASp_NAfK_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_sigmoid(z):\n",
    "  val=sigmoid(z)\n",
    "  assert(val==0.8807970779778823)\n",
    "  return True\n",
    "grader_sigmoid(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gS7JXbcrBOFF"
   },
   "source": [
    "<font color='blue'> Compute loss </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfEiS22zBVYy"
   },
   "source": [
    "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(y_true,y_pred):\n",
    "    '''In this function, we will compute log loss '''\n",
    "    loss=[]\n",
    "    for i in range((len(y_true))):\n",
    "        if y_pred[i] < 0.5:\n",
    "            l = (1-y_true[i])*math.log10(1-y_pred[i])\n",
    "            loss.append(l)\n",
    "        else:\n",
    "            l = y_true[i]*math.log10(y_pred[i])\n",
    "            loss.append(l)\n",
    "    loss = (-1 * 1/len(loss) * sum(loss))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zs1BTXVSClBt"
   },
   "source": [
    "<font color='cyan'>Grader function - 3 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzttjvBFCuQ5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_logloss(true,pred):\n",
    "  loss=logloss(true,pred)\n",
    "  assert(loss==0.07644900402910389)\n",
    "  return True\n",
    "true=[1,1,0,1,0]\n",
    "pred=[0.9,0.8,0.1,0.8,0.2]\n",
    "grader_logloss(true,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQabIadLCBAB"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to  'w' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTMxiYKaCQgd"
   },
   "source": [
    "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMVikyuFsKo5"
   },
   "outputs": [],
   "source": [
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gradient w.r.to w '''\n",
    "    dw =x*(y-sigmoid(np.dot(w,x)+b)) - alpha/N * w\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUFLNqL_GER9"
   },
   "source": [
    "<font color='cyan'>Grader function - 4 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI3xD8ctGEnJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_dw(x,y,w,b,alpha,N):\n",
    "  grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
    "  assert(np.sum(grad_dw)==2.613689585)\n",
    "  return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LE8g84_GI62n"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to 'b' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fHvTYZzZJJ_N"
   },
   "source": [
    "$ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nUf2ft4EZp8"
   },
   "outputs": [],
   "source": [
    " def gradient_db(x,y,w,b):\n",
    "    '''In this function, we will compute gradient w.r.to b '''\n",
    "    db =(y-sigmoid(np.dot(w,x)+b))\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbcBzufVG6qk"
   },
   "source": [
    "<font color='cyan'>Grader function - 5 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TfFDKmscG5qZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_db(x,y,w,b):\n",
    "  grad_db=gradient_db(x,y,w,b)\n",
    "  assert(grad_db==-0.5)\n",
    "  return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_db(grad_x,grad_y,grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(w,b, X):\n",
    "    N = len(X.tolist())\n",
    "    predict = []\n",
    "\n",
    "    for i in range(N):\n",
    "        z = np.dot(X_train[i],w) + b\n",
    "        predict.append(sigmoid(z))\n",
    "\n",
    "    return np.array(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCK0jY_EOvyU"
   },
   "source": [
    "<font color='blue'> Implementing logistic regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmAdc5ejEZ25"
   },
   "outputs": [],
   "source": [
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
    "\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    scale_down_factor = 0.0001\n",
    "    epoch = 1\n",
    "    w, b = initialize_weights(X_train[0])\n",
    "    wl = []\n",
    "    bl = []\n",
    "    \n",
    "    Lw=np.zeros_like(X_train[0])\n",
    "    Lb=0\n",
    "    \n",
    "    loss = 0\n",
    "    prev = 0\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    while epoch <= epochs:\n",
    "        \n",
    "        y_train_pred = []\n",
    "        y_test_pred = []\n",
    "        np.random.RandomState(seed=2)\n",
    "        \n",
    "        for m in range(len(X_train)):\n",
    "            \n",
    "            i = np.random.choice(len(X_train))\n",
    "            z = np.dot(X_train[i],w) + b\n",
    "            \n",
    "            Lw = gradient_dw(X_train[i],y_train[i],w,b,alpha,len(X_train))\n",
    "            Lb = gradient_db(X_train[i],y_train[i],w,b)\n",
    "            \n",
    "            w=(1-(alpha * scale_down_factor/epochs))*w+alpha*Lw\n",
    "            b=b+alpha*Lb\n",
    "            \n",
    "        train_loss.append(round(logloss(y_train, pred(w,b,X_train)),3))\n",
    "        test_loss.append(round(logloss(y_test,pred(w,b,X_test)),3))\n",
    "        \n",
    "        if train_loss[-1] == prev:\n",
    "            break;\n",
    "        else:\n",
    "            prev = train_loss[-1]\n",
    "            print(\"Epoch: %d, train_Loss: %.3f, test_Loss: %.3f\" %(epoch, train_loss[-1], test_loss[-1]))\n",
    "            epoch+=1\n",
    "        \n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(train_loss, label='train_log_loss')\n",
    "    plt.plot(test_loss, label='test_log_loss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.title('Log loss vs epoch')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('log loss')\n",
    "    plt.show()\n",
    "        \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUquz7LFEZ6E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_Loss: 0.134, test_Loss: 0.100\n",
      "Epoch: 2, train_Loss: 0.107, test_Loss: 0.081\n",
      "Epoch: 3, train_Loss: 0.093, test_Loss: 0.071\n",
      "Epoch: 4, train_Loss: 0.085, test_Loss: 0.066\n",
      "Epoch: 5, train_Loss: 0.080, test_Loss: 0.062\n",
      "Epoch: 6, train_Loss: 0.076, test_Loss: 0.060\n",
      "Epoch: 7, train_Loss: 0.073, test_Loss: 0.057\n",
      "Epoch: 8, train_Loss: 0.071, test_Loss: 0.056\n",
      "Epoch: 9, train_Loss: 0.070, test_Loss: 0.055\n",
      "Epoch: 10, train_Loss: 0.068, test_Loss: 0.053\n",
      "Epoch: 11, train_Loss: 0.067, test_Loss: 0.053\n",
      "Epoch: 12, train_Loss: 0.066, test_Loss: 0.052\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "N=len(X_train)\n",
    "epochs=50\n",
    "w,b=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.88182352,  0.62528137, -0.07278131,  0.63411476, -0.36473866,\n",
       "         0.93301397, -0.8969611 , -0.07015339,  0.40339115,  0.40773854,\n",
       "         0.24340665,  0.05183877, -0.08646485,  0.53592828,  0.0728324 ]),\n",
       " -1.2900732161123047)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4Zf_wPARlwY"
   },
   "source": [
    "<font color='red'>Goal of assignment</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3eF_VSPSH2z"
   },
   "source": [
    "Compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nx8Rs9rfEZ1R"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.00824831, -0.00634226,  0.00316014,  0.00304369,  0.01960509,\n",
       "          0.00066154, -0.00122589,  0.00325182, -0.00252302, -0.01226056,\n",
       "         -0.00381478,  0.00137678,  0.00231501, -0.00488824,  0.00639352]]),\n",
       " array([0.01573216]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
    "w-clf.coef_, b-clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "230YbSgNSUrQ"
   },
   "source": [
    "<font color='blue'>Plot epoch number vs train , test loss </font>\n",
    "\n",
    "* epoch number on X-axis\n",
    "* loss on Y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUN8puFoEZtU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9517866666666667\n",
      "0.94888\n"
     ]
    }
   ],
   "source": [
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(0)\n",
    "    return np.array(predict)\n",
    "\n",
    "print(1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
    "print(1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
