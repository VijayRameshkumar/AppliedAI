{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"8D_LR_SVM.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"86Tvnj5UblTy","colab_type":"text"},"source":["## Task-D: Collinear features and their effect on linear models"]},{"cell_type":"code","metadata":{"id":"qn_eOn2EblT3","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.model_selection import GridSearchCV\n","import seaborn as sns\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VMoYWIayblUB","colab_type":"code","colab":{}},"source":["data = pd.read_csv('task_d.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RfStXG4tblUI","colab_type":"code","outputId":"ddf4eec6-7f53-4d28-914f-23133957d6d5","colab":{}},"source":["data.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>x</th>\n","      <th>y</th>\n","      <th>z</th>\n","      <th>x*x</th>\n","      <th>2*y</th>\n","      <th>2*z+3*x*x</th>\n","      <th>w</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.581066</td>\n","      <td>0.841837</td>\n","      <td>-1.012978</td>\n","      <td>-0.604025</td>\n","      <td>0.841837</td>\n","      <td>-0.665927</td>\n","      <td>-0.536277</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.894309</td>\n","      <td>-0.207835</td>\n","      <td>-1.012978</td>\n","      <td>-0.883052</td>\n","      <td>-0.207835</td>\n","      <td>-0.917054</td>\n","      <td>-0.522364</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-1.207552</td>\n","      <td>0.212034</td>\n","      <td>-1.082312</td>\n","      <td>-1.150918</td>\n","      <td>0.212034</td>\n","      <td>-1.166507</td>\n","      <td>0.205738</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-1.364174</td>\n","      <td>0.002099</td>\n","      <td>-0.943643</td>\n","      <td>-1.280666</td>\n","      <td>0.002099</td>\n","      <td>-1.266540</td>\n","      <td>-0.665720</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.737687</td>\n","      <td>1.051772</td>\n","      <td>-1.012978</td>\n","      <td>-0.744934</td>\n","      <td>1.051772</td>\n","      <td>-0.792746</td>\n","      <td>-0.735054</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          x         y         z       x*x       2*y  2*z+3*x*x         w  \\\n","0 -0.581066  0.841837 -1.012978 -0.604025  0.841837  -0.665927 -0.536277   \n","1 -0.894309 -0.207835 -1.012978 -0.883052 -0.207835  -0.917054 -0.522364   \n","2 -1.207552  0.212034 -1.082312 -1.150918  0.212034  -1.166507  0.205738   \n","3 -1.364174  0.002099 -0.943643 -1.280666  0.002099  -1.266540 -0.665720   \n","4 -0.737687  1.051772 -1.012978 -0.744934  1.051772  -0.792746 -0.735054   \n","\n","   target  \n","0       0  \n","1       0  \n","2       0  \n","3       0  \n","4       0  "]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"JIIuomCkblUP","colab_type":"code","colab":{}},"source":["X = data.drop(['target'], axis=1).values\n","Y = data['target'].values"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ydm98u3EblUU","colab_type":"text"},"source":["### Doing perturbation test to check the presence of collinearity  \n","\n","#### Task: 1 Logistic Regression\n","<pre>\n","\n","\n","1. <b>Finding the Correlation between the features</b>\n","    a. check the correlation between the features\n","    b. plot heat map of correlation matrix using seaborn heatmap\n","2. <b>Finding the best model for the given data</b>\n","    a. Train Logistic regression on data(X,Y) that we have created in the above cell\n","    b. Find the best hyper prameter alpha with hyper parameter tuning using k-fold cross validation (grid search CV or         \n","    random search CV make sure you choose the alpha in log space)\n","    c. Creat a new Logistic regression with the best alpha\n","    (search for how to get the best hyper parameter value), name the best model as 'best_model'\n","    \n","3. <b>Getting the weights with the original data</b>\n","    a. train the 'best_model' with X, Y\n","    b. Check the accuracy of the model 'best_model_accuracy'\n","    c. Get the weights W using best_model.coef_\n","\n","4. <b>Modifying original data</b>\n","    a. Add a noise(order of 10^-2) to each element of X \n","    and get the new data set X' (X' = X + e)\n","    b. Train the same 'best_model' with data (X', Y)\n","    c. Check the accuracy of the model 'best_model_accuracy_edited'\n","    d. Get the weights W' using best_model.coef_\n","    \n","5. <b> Checking deviations in metric and weights </b>\n","    a. find the difference between 'best_model_accuracy_edited' and 'best_model_accuracy'\n","    b. find the absolute change between each value of W and W' ==> |(W-W')|\n","    c. print the top 4 features which have higher % change in weights \n","    compare to the other feature\n","\n","</pre>\n","\n","#### Task: 2 Linear SVM\n","\n","<pre>\n","1. Do the same steps (2, 3, 4, 5) we have done in the above task 1.\n","</pre>\n","\n","<strong><font color='red'>Do write the observations based on the results you get from the deviations of weights in both Logistic Regression and linear SVM</font></strong>"]},{"cell_type":"code","metadata":{"id":"Lai8wXU1pmSb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}