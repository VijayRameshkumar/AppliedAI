{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RandomForest-Assignment.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1wb6wXDlgT7vcJcWPAbkXMmSB0jNXS0Sk",
      "authorship_tag": "ABX9TyPRRjOXgKyXJ+5GU5Ui9FJ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VijayRameshkumar/AppliedAI/blob/main/09_DecisionTree/practice/RandomForest_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PMnArDjeUIj"
      },
      "source": [
        "!pip install pandas==1.1.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J3ZyGxzecaJ"
      },
      "source": [
        "import pickle\n",
        "\n",
        "glove_vectors_path = '/content/drive/MyDrive/6_Donors_choose_NB/glove_vectors'\n",
        "preprocessed_data = '/content/drive/MyDrive/6_Donors_choose_NB/preprocessed_data.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRBhXYHde8lc"
      },
      "source": [
        "# **TASK 1:** \n",
        "\n",
        "##**`RandomForest Classifier`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phyGzv0-e0ho"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.width', 10)\n",
        "pd.set_option('display.max_colwidth', 10)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS87vPztglnU"
      },
      "source": [
        "## **1.1 LoadingData**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaOVQ79hfiK5"
      },
      "source": [
        "data = pd.read_csv(preprocessed_data)\n",
        "data = pd.concat([data.loc[data['project_is_approved'] == 0], data.loc[data['project_is_approved'] == 1].head(33458)], sort=False)\n",
        "\n",
        "target = data['project_is_approved']\n",
        "data = data.drop(columns=['project_is_approved'])\n",
        "\n",
        "data.info() #basic info about dataset : To know how many categorical and numeric data point"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwOQftDIMvEn"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, ytrain, ytest = train_test_split(data, target, test_size=0.33, stratify=target)\n",
        "\n",
        "del data\n",
        "del target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jto8K9wDMjhG"
      },
      "source": [
        "## **1.2 TextFeatures Encoding**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LLZlenhbOBg"
      },
      "source": [
        "### **1.2.1 Tfidf Vectorization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwt2zfUHqfkK"
      },
      "source": [
        "############### TFIDF - Vectorizer ######################\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(min_df=10, max_features=5000, ngram_range=(1,2))\n",
        "tfidf.fit(X_train.essay.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMY4JxddRdjP"
      },
      "source": [
        "### **1.2.2 Tfidf-W2V vectorizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_khjlWeRciY"
      },
      "source": [
        "################# TFIDF - W2V ###########################\n",
        "import tqdm\n",
        "\n",
        "with open(glove_vectors_path, 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "    glove_words =  set(model.keys())\n",
        "\n",
        "def tfidf_w2v(essay):\n",
        "    tfidf = TfidfVectorizer(min_df=10, max_features=5000, ngram_range=(1,2))\n",
        "    tfidf.fit(essay.values)\n",
        "\n",
        "    dictionary = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
        "    tfidf_words = set(tfidf.get_feature_names())\n",
        "\n",
        "    tfidf_w2v_vectors = []\n",
        "\n",
        "    for sentence in essay:\n",
        "        vector = np.zeros(300)\n",
        "        tfidf_weight = 0\n",
        "        for word in sentence.split():\n",
        "            if (word in glove_words) and (word in tfidf_words):\n",
        "                vec = model[word]\n",
        "                tfidf_vec = dictionary[word] * sentence.count(word) / len(sentence.split())\n",
        "                vector += vec * tfidf_vec\n",
        "                tfidf_weight += tfidf_vec\n",
        "                \n",
        "        if tfidf_weight != 0:\n",
        "            vector /= tfidf_vec\n",
        "        tfidf_w2v_vectors.append(vector)\n",
        "\n",
        "    return np.array(tfidf_w2v_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-h7LSaTidnf"
      },
      "source": [
        "## **1.3 Categorical Feature Encoding & Normalize Numeric Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANpHmnWtie7T"
      },
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "ohe1 = CountVectorizer()\n",
        "ohe1.fit(X_train['school_state'].values)\n",
        "school_state_ohe_train = ohe1.transform(X_train['school_state'].values).toarray() # fit has to happen only on train data\n",
        "school_state_ohe_test = ohe1.transform(X_test['school_state'].values).toarray()\n",
        "\n",
        "ohe2 = CountVectorizer()\n",
        "ohe2.fit(X_train['teacher_prefix'].values)\n",
        "teacher_prefix_ohe_train = ohe2.transform(X_train['teacher_prefix'].values).toarray() # fit has to happen only on train data\n",
        "teacher_prefix_ohe_test = ohe2.transform(X_test['teacher_prefix'].values).toarray()\n",
        "\n",
        "ohe3 = CountVectorizer()\n",
        "ohe3.fit(X_train['project_grade_category'].values)\n",
        "project_grade_category_ohe_train = ohe3.transform(X_train['project_grade_category'].values).toarray() # fit has to happen only on train data\n",
        "project_grade_category_ohe_test = ohe3.transform(X_test['project_grade_category'].values).toarray()\n",
        "\n",
        "ohe4 = CountVectorizer()\n",
        "ohe4.fit(X_train['clean_categories'].values)\n",
        "clean_categories_ohe_train = ohe4.transform(X_train['clean_categories'].values).toarray() # fit has to happen only on train data\n",
        "clean_categories_ohe_test = ohe4.transform(X_test['clean_categories'].values).toarray()\n",
        "\n",
        "ohe5 = CountVectorizer()\n",
        "ohe5.fit(X_train['clean_subcategories'].values)\n",
        "clean_subcategories_ohe_train = ohe5.transform(X_train['clean_subcategories'].values).toarray() # fit has to happen only on train data\n",
        "clean_subcategories_ohe_test = ohe5.transform(X_test['clean_subcategories'].values).toarray()\n",
        "\n",
        "norm = Normalizer()\n",
        "norm.fit(X_train['price'].values.reshape(-1,1))\n",
        "X_train_price_norm = norm.transform(X_train['price'].values.reshape(-1,1))\n",
        "X_test_price_norm = norm.transform(X_test['price'].values.reshape(-1,1))\n",
        "\n",
        "norm.fit(X_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\n",
        "X_train_no_prev_proj = norm.transform(X_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\n",
        "X_test_no_prev_proj = norm.transform(X_test['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp7y_-3IQXvb"
      },
      "source": [
        "import gc\n",
        "\n",
        "X_train_essay = X_train['essay']\n",
        "X_test_essay = X_test['essay']\n",
        "\n",
        "X_train = np.column_stack((X_train_no_prev_proj, X_train_price_norm, school_state_ohe_train, teacher_prefix_ohe_train, project_grade_category_ohe_train, clean_categories_ohe_train, clean_subcategories_ohe_train))\n",
        "X_test = np.column_stack((X_test_no_prev_proj, X_test_price_norm, school_state_ohe_test, teacher_prefix_ohe_test, project_grade_category_ohe_test, clean_categories_ohe_test, clean_subcategories_ohe_test))\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "print(\"Final Data matrix\")\n",
        "print(X_train.shape, ytrain.shape)\n",
        "print(X_test.shape, ytest.shape)\n",
        "\n",
        "print(\"=\"*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV0TXy16itAy"
      },
      "source": [
        "## **1.4 Sentiment Analyser**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pxFFYsfio5Z"
      },
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "def sentiment_anayser(essay):\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    negative = []\n",
        "    positive = []\n",
        "    neutral = []\n",
        "    for sentence in essay:\n",
        "        ss = sid.polarity_scores(sentence)\n",
        "        sentmnt = list(ss.values())\n",
        "        neg = sentmnt[0]\n",
        "        neu = sentmnt[1]\n",
        "        pos = sentmnt[2]\n",
        "        negative.append(neg)\n",
        "        neutral.append(neu)\n",
        "        positive.append(pos)\n",
        "    return np.column_stack((np.array(negative), np.array(neutral), np.array(positive)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMuiEVi1dZRh"
      },
      "source": [
        "## **1.5 Hyper_Parameter Tuning** (SET1 & SET2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeO4Rt6zi-6c"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "sets = ['tfidf_w2v_vectors', 'tfidf']\n",
        "scorer = dict()\n",
        "\n",
        "X_train = np.column_stack((X_train, sentiment_anayser(X_train_essay)))\n",
        "\n",
        "for index, df_set in enumerate(sets):\n",
        "\n",
        "    if df_set == 'tfidf':\n",
        "        temp = tfidf.transform(X_train_essay).toarray()\n",
        "        X = np.column_stack((X_train, temp))\n",
        "        \n",
        "    elif df_set == 'tfidf_w2v_vectors': \n",
        "        temp = tfidf_w2v(X_train_essay)\n",
        "        X = np.column_stack((X_train, temp))\n",
        "    \n",
        "    print(\"Final Data matrix\")\n",
        "    print(X.shape, ytrain.shape)\n",
        "\n",
        "    parameters={'max_depth' : [1, 5, 10, 50], 'min_samples_split' : [5, 10, 100, 500]}\n",
        "\n",
        "    gsc=GridSearchCV(estimator=RandomForestClassifier(random_state=2),\n",
        "                     param_grid=parameters, scoring='roc_auc', verbose=1, n_jobs=2, return_train_score=True)\n",
        "    \n",
        "    grid_result = gsc.fit(X, ytrain)\n",
        "    scorer[df_set] = grid_result.cv_results_\n",
        "\n",
        "    print(\"#\"*50,\"\\n\\n\")\n",
        "    print(\"\\n\", df_set, \" : \", \"\\n\")\n",
        "\n",
        "    best_params=grid_result.best_params_\n",
        "    print(best_params)\n",
        "\n",
        "    print(grid_result.best_score_,\"\\n\")\n",
        "    print(\"#\"*50,\"\\n\\n\")\n",
        "\n",
        "    del X\n",
        "    del temp\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiTji9MXX7Oh"
      },
      "source": [
        "scorer['tfidf_w2v_vectors']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LF93imBezlz"
      },
      "source": [
        "scorer['tfidf']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_a1D7lclzqI"
      },
      "source": [
        "## **1.6 Cross-Validation Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7rtkemHe2zq"
      },
      "source": [
        "mean_test_score = [0.65820944, 0.65820944, 0.65820944, 0.65820944, 0.68122492, 0.68113072, 0.68077205, 0.68077527, \n",
        "                   0.69357459, 0.69372878, 0.69341197, 0.69277492, 0.70191902, 0.70235819, 0.70424156, 0.70644806]\n",
        "\n",
        "mean_train_score = [0.66895523, 0.66895523, 0.66895523, 0.66895523, 0.73153556, 0.73019983, 0.72432353, 0.71632896, \n",
        "                    0.82714781, 0.82142404, 0.7903215 , 0.75914939, 0.9999747 , 0.99989934, 0.99099531, 0.91503008]\n",
        "\n",
        "params = [{'max_depth': 1, 'min_samples_split': 5},\n",
        "  {'max_depth': 1, 'min_samples_split': 10},\n",
        "  {'max_depth': 1, 'min_samples_split': 100},\n",
        "  {'max_depth': 1, 'min_samples_split': 500},\n",
        "  {'max_depth': 5, 'min_samples_split': 5},\n",
        "  {'max_depth': 5, 'min_samples_split': 10},\n",
        "  {'max_depth': 5, 'min_samples_split': 100},\n",
        "  {'max_depth': 5, 'min_samples_split': 500},\n",
        "  {'max_depth': 10, 'min_samples_split': 5},\n",
        "  {'max_depth': 10, 'min_samples_split': 10},\n",
        "  {'max_depth': 10, 'min_samples_split': 100},\n",
        "  {'max_depth': 10, 'min_samples_split': 500},\n",
        "  {'max_depth': 50, 'min_samples_split': 5},\n",
        "  {'max_depth': 50, 'min_samples_split': 10},\n",
        "  {'max_depth': 50, 'min_samples_split': 100},\n",
        "  {'max_depth': 50, 'min_samples_split': 500}]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaVjGEhlqezl"
      },
      "source": [
        "max_depths = []\n",
        "min_samples_split = []\n",
        "\n",
        "for parameter in params:\n",
        "    max_depths.append(parameter['max_depth'])\n",
        "    min_samples_split.append(parameter['min_samples_split'])\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['max_depth'] = pd.Series(max_depths)\n",
        "df['min_samples_split'] = pd.Series(min_samples_split)\n",
        "df['mean_test_score'] = mean_test_score\n",
        "df['mean_train_score'] = mean_train_score\n",
        "\n",
        "train_heatmap = df.pivot(index='min_samples_split', columns='max_depth', values='mean_train_score')\n",
        "test_heatmap = df.pivot(index='min_samples_split', columns='max_depth', values='mean_test_score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L85Jb3rqsrX"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(train_heatmap, annot=True)\n",
        "plt.title(\"Train_Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "sns.heatmap(test_heatmap, annot=True)\n",
        "plt.title(\"Test_Heatmap\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRqGTbsLrMzo"
      },
      "source": [
        "## **1.7 Model Train & Predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jX6j2qYqvAN"
      },
      "source": [
        "X_train = np.column_stack((X_train, sentiment_anayser(X_train_essay)))\n",
        "temp = tfidf.transform(X_train_essay).toarray()\n",
        "X_train = np.column_stack((X_train, temp))\n",
        "\n",
        "X_test = np.column_stack((X_test, sentiment_anayser(X_test_essay)))\n",
        "temp = tfidf.transform(X_test_essay).toarray()\n",
        "X_test = np.column_stack((X_test, temp))\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLLdCDHyuyHS"
      },
      "source": [
        "model = RandomForestClassifier(max_depth = 50, min_samples_split= 500, random_state=2)\n",
        "\n",
        "model = model.fit(X_train, ytrain)\n",
        "Y_pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvCN84n0s2BQ"
      },
      "source": [
        "## **1.8 ConfusionMatrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GwhukAvsg6d"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "font = {\n",
        "'family' : 'DejaVu Sans',\n",
        "'weight' : 'bold',\n",
        "'size' : '16'\n",
        "}\n",
        "\n",
        "plt.rc('font', **font)\n",
        "mat = confusion_matrix(ytest, Y_pred)\n",
        "plot_confusion_matrix(conf_mat=mat, figsize=(5,5), show_normed=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpaTa9prtBGP"
      },
      "source": [
        "## **1.9 AUC-Plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7irwX5Y5srMa"
      },
      "source": [
        "from sklearn.metrics import auc\n",
        "\n",
        "print(\"train_roc_auc_score : \" , roc_auc_score(ytrain, model.predict(X_train)), '\\n')\n",
        "print(\"test_roc_auc_score : \", roc_auc_score(ytest, Y_pred), '\\n')\n",
        "\n",
        "probs = model.predict_proba(X_train)\n",
        "probs = probs[:, 1]\n",
        "\n",
        "train_fpr, train_tpr, train_thresholds = roc_curve(ytrain, probs)\n",
        "\n",
        "probs = model.predict_proba(X_test)\n",
        "probs = probs[:, 1]\n",
        "\n",
        "test_fpr, test_tpr, test_thresholds = roc_curve(ytest, probs)\n",
        "\n",
        "print(\"train_auc_score : \" , auc(train_fpr, train_tpr), '\\n')\n",
        "print(\"test_auc_score : \", auc(test_fpr, test_tpr), '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM70MOKBtRpo"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"ticks\")\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "print(\"train_auc_score : \" , auc(train_fpr, train_tpr), \"\\n\\n\")\n",
        "print(\"test_auc_score : \", auc(test_fpr, test_tpr), \"\\n\\n\")\n",
        "\n",
        "plt.plot(train_fpr, train_tpr, color='orange', label='_train_ROC')\n",
        "plt.plot(test_fpr, test_tpr, color='green', label='_test_ROC')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(['train_AUC', 'test_AUC', 'AUC_Boundary'])\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCfYj2M1tdvo"
      },
      "source": [
        "## **1.10 False-Positive Prediction WordCloud**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thu0cfxmtVkp"
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "Y_pred = Y_pred.tolist()\n",
        "\n",
        "ytest = ytest.tolist()\n",
        "\n",
        "false_positive = []\n",
        "\n",
        "for index in range(len(Y_pred)):\n",
        "    if ytest[index] == 0 and Y_pred[index] == 1:\n",
        "        false_positive.append(index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYwNIi9Uvdp5"
      },
      "source": [
        "data = pd.read_csv(preprocessed_data)\n",
        "data = pd.concat([data.loc[data['project_is_approved'] == 0], data.loc[data['project_is_approved'] == 1].head(33458)], sort=False)\n",
        "data = data.drop(columns=['project_is_approved'])\n",
        "\n",
        "fp_essay = data.iloc[false_positive]['essay']\n",
        "fp_price = data.iloc[false_positive]['price']\n",
        "fp_teacher_number_of_previously_posted_projects = data.iloc[false_positive]['teacher_number_of_previously_posted_projects']\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "word_cloud = []\n",
        "print('#'*50, '\\n', 'WORDS IN ESSAYS - FOR FALSE-POSITIVE PREDICTIONS', '\\n', '#'*50)\n",
        "comment_words = \"\"\n",
        "\n",
        "for sentence in fp_essay:\n",
        "    for words in sentence.split():\n",
        "        word_cloud.append(words.lower())\n",
        "        \n",
        "comment_words += \" \".join(word_cloud)+\" \"\n",
        "\n",
        "wordcloud = WordCloud(width = 800, height = 800, \n",
        "                background_color ='white', \n",
        "                stopwords = stopwords, \n",
        "                min_font_size = 10).generate(comment_words) \n",
        "  \n",
        "# plot the WordCloud image                        \n",
        "plt.figure(figsize = (10, 10), facecolor = None) \n",
        "plt.imshow(wordcloud) \n",
        "plt.axis(\"off\") \n",
        "plt.tight_layout(pad = 0) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs3OsJl8wKlN"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "sns.boxplot(fp_price)\n",
        "\n",
        "plt.title(\"FALSE-POSITIVE PRICE\")\n",
        "plt.legend([\"FALSE-POSITIVE PRICE\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOEA5LKYwwht"
      },
      "source": [
        "sns.set_style(\"whitegrid\");\n",
        "plt.figure(figsize=(20,5))\n",
        "\n",
        "plt.rcParams['axes.titlesize'] = 20\n",
        "plt.rcParams['axes.titleweight'] = 10\n",
        "\n",
        "count, bin_edges = np.histogram(fp_teacher_number_of_previously_posted_projects, bins=10, density=True)\n",
        "\n",
        "nodes_pdf = count / sum(count)\n",
        "nodes_cdf = np.cumsum(nodes_pdf)\n",
        "\n",
        "plt.plot(bin_edges[1:],nodes_pdf, color='green', marker='o', linestyle='solid')\n",
        "\n",
        "plt.title(\"PDF - FALSE POSITIVE teacher_number_of_previously_posted_projects\\n\")\n",
        "plt.legend(['FP - teacher_no.of_prev_posted_projects'])\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk-A5yNQxCLW"
      },
      "source": [
        "# **TASK 2:** \n",
        "\n",
        "##**`DecisionTree Classifier`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxUT5jQWw2r4"
      },
      "source": [
        "print(X_train.shape, ytrain.shape, X_test.shape, np.array(ytest).shape) #SET 1 - TFIDF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jndVhXsB5TGr"
      },
      "source": [
        "## **2.1 Non-Zero Feature Importance Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbnjefmQ5SgH"
      },
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=2)\n",
        "model = model.fit(X_train, ytrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv4GYLJUInKh"
      },
      "source": [
        "non_zero_Feature_importance_index = [index for index, value in enumerate(model.feature_importances_.tolist()) if value != float(0.0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQnhJXWOHU1K"
      },
      "source": [
        "print(len(non_zero_Feature_importance_index))\n",
        "\n",
        "X = np.take(X_train, non_zero_Feature_importance_index, axis=1)\n",
        "X_ = np.take(X_test, non_zero_Feature_importance_index, axis=1)\n",
        "\n",
        "print(X.shape, X_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EFDbvZ304oW"
      },
      "source": [
        "## **2.2 Hyper_Parameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBVuDQm3xVu3"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters={'max_depth' : [1, 5, 10, 50, 'None'], 'min_samples_split' : [2, 5, 10, 100, 500]}\n",
        "\n",
        "gsc=GridSearchCV(estimator=DecisionTreeClassifier(random_state=2),\n",
        "                    param_grid=parameters, scoring='roc_auc', verbose=1, n_jobs=-1, return_train_score=True)\n",
        "\n",
        "grid_result = gsc.fit(X, ytrain)\n",
        "\n",
        "print(\"#\"*50,\"\\n\\n\")\n",
        "best_params=grid_result.best_params_\n",
        "print(best_params,'\\n')\n",
        "print(grid_result.best_score_,\"\\n\")\n",
        "print(\"#\"*50,\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_px1b7R6yW2I"
      },
      "source": [
        "grid_result.cv_results_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obUnpUR31Dvr"
      },
      "source": [
        "## **2.3 Cross-Validation Result**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lUpkwhezOCh"
      },
      "source": [
        "params = grid_result.cv_results_['params']\n",
        "mean_train_score = pd.Series(grid_result.cv_results_['mean_train_score'])\n",
        "mean_test_score = pd.Series(grid_result.cv_results_['mean_test_score'])\n",
        "\n",
        "min_samples_split = []\n",
        "max_depth = []\n",
        "for parameter in params:\n",
        "    min_samples_split.append(parameter['min_samples_split'])\n",
        "    max_depth.append(parameter['max_depth'])\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['min_samples_split'] = pd.Series(min_samples_split)\n",
        "df['mean_test_score'] = mean_test_score\n",
        "df['mean_train_score'] = mean_train_score\n",
        "df['max_depth'] = pd.Series(max_depth)\n",
        "\n",
        "train_heatmap = df.pivot(index='min_samples_split', columns='max_depth', values='mean_train_score')\n",
        "test_heatmap = df.pivot(index='min_samples_split', columns='max_depth', values='mean_test_score')\n",
        "\n",
        "sns.heatmap(train_heatmap, annot=True)\n",
        "plt.title(\"Train_Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "sns.heatmap(test_heatmap, annot=True)\n",
        "plt.title(\"Test_Heatmap\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djqvofUNBH7p"
      },
      "source": [
        "print(X_train.shape, ytrain.shape, X_test.shape, np.array(ytest).shape) #SET 1 - TFIDF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABK3LXNKMThG"
      },
      "source": [
        "## **2.4 Model Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFTSdDETMTJz"
      },
      "source": [
        "model = DecisionTreeClassifier(random_state=2, max_depth=10, min_samples_split=500)\n",
        "model.fit(X, ytrain)\n",
        "Y_pred = model.predict(X_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6H9TdJN1Ovl"
      },
      "source": [
        "## **2.5 ConfusionMatrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IMydevnzl6P"
      },
      "source": [
        "font = {\n",
        "'family' : 'DejaVu Sans',\n",
        "'weight' : 'bold',\n",
        "'size' : '16'\n",
        "}\n",
        "\n",
        "plt.rc('font', **font)\n",
        "mat = confusion_matrix(ytest, Y_pred)\n",
        "plot_confusion_matrix(conf_mat=mat, figsize=(5,5), show_normed=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aet3xArE1fCm"
      },
      "source": [
        "## **2.6 AUC-Plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrYtHOxLzwXo"
      },
      "source": [
        "from sklearn.metrics import auc\n",
        "\n",
        "print(\"train_roc_auc_score : \" , roc_auc_score(ytrain, model.predict(X)),'\\n')\n",
        "print(\"test_roc_auc_score : \", roc_auc_score(ytest, Y_pred), '\\n')\n",
        "\n",
        "probs = model.predict_proba(X)\n",
        "probs = probs[:, 1]\n",
        "train_fpr, train_tpr, train_thresholds = roc_curve(ytrain, probs)\n",
        "\n",
        "probs = model.predict_proba(X_)\n",
        "probs = probs[:, 1]\n",
        "test_fpr, test_tpr, test_thresholds = roc_curve(ytest, probs)\n",
        "\n",
        "print(\"train_auc_score : \" , auc(train_fpr, train_tpr), '\\n')\n",
        "print(\"test_auc_score : \", auc(test_fpr, test_tpr), '\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liIKkzo70ADn"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"ticks\")\n",
        "sns.set(style='darkgrid')\n",
        "print(\"train_auc_score : \" , auc(train_fpr, train_tpr), \"\\n\\n\")\n",
        "print(\"test_auc_score : \", auc(test_fpr, test_tpr), \"\\n\\n\")\n",
        "\n",
        "plt.plot(train_fpr, train_tpr, color='orange', label='_train_ROC')\n",
        "plt.plot(test_fpr, test_tpr, color='green', label='_test_ROC')\n",
        "plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(['train_AUC', 'test_AUC', 'AUC_Boundary'])\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syr0jqgI1opU"
      },
      "source": [
        "# **`RESULT`**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUe3IPpU0F9Z"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "x = PrettyTable()\n",
        "x.field_names = [\"Vectorizer\", \"Model\", \"Hyper_Parameter\", \"Train_AUC\", \"Test_AUC\"]\n",
        "x.add_row([\"TFIDF-W2V\", 'RandomForest', 'max_depth : 50, min_samples_split : 100', 0.93, 0.68])\n",
        "x.add_row([\"TFIDF\", 'RandomForest', 'max_depth : 50, min_samples_split : 500', 0.92, 0.71])\n",
        "x.add_row([\"TFIDF\", 'DecisionTree', 'max_depth : 10, min_samples_split : 500', 0.69, 0.65])\n",
        "\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}